{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d99750",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./images/DLI_Header.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bd444",
   "metadata": {},
   "source": [
    "# Scaling CUDA C++ Applications on Multiple Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908ec20",
   "metadata": {},
   "source": [
    "Welcome to _Scaling CUDA C++ Applications on Multiple Nodes_. In this course you will learn several techniques for scaling single GPU CUDA applications to multiple GPUs and multiple nodes, with an emphasis on [NVSHMEM](https://developer.nvidia.com/nvshmem) which allows for elegant multi GPU application code and has been proven to scale very well on systems with many GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34c905",
   "metadata": {},
   "source": [
    "## The Coding Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1317b",
   "metadata": {},
   "source": [
    "For your work today, you have access to several GPUs in the cloud. Run the following cell to see the GPUs available to you today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5427ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 19 16:21:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   37C    P0    41W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   40C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000003:00:00.0 Off |                  Off |\n",
      "| N/A   37C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000004:00:00.0 Off |                  Off |\n",
      "| N/A   39C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a9c09",
   "metadata": {},
   "source": [
    "While your work today will be on a single node, all the techniques you learn today, in particular CUDA-aware MPI and NVSHMEM, can be used to run your applications across clusters of multi GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8318f6c",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8b050",
   "metadata": {},
   "source": [
    "During the workshop today you will work through each of the following notebooks with your instructor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87104df7",
   "metadata": {},
   "source": [
    "- [_Monte Carlo Approximation of  ùúã  - Single GPU_](02_MCœÄ-SGPU.ipynb): You will begin by familiarizing yourself with a single GPU implementation of the monte-carlo approximation of œÄ algorithm, which we will use to introduce many multi GPU programming paradigms.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs_](03_MCœÄ-MGPU.ipynb): In this notebook you will extend the monte-carlo œÄ program to run on multiple GPUs by looping over available GPU devices.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs with Peer Access_](04_MCœÄ-P2P.ipynb): In this notebook you will improve on your multi GPU code by utilizing direct peer-to-peer GPU communication.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - MPI_](05_MCœÄ-MPI.ipynb): In this notebook you will be introduced to the single-program multiple-data paradigm (SPMD) and will simplify your monte-carlo œÄ application with MPI.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - CUDA-Aware MPI_](06_MCœÄ-CUDA-MPI.ipynb): In this notebook you will learn about CUDA-Aware MPI, which facilitates direct peer-to-peer communication between GPUs in the SPMD paradigm.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM_](07_MCœÄ-NVSHMEM-Dup.ipynb): In this notebook you will be introduced to NVSHMEM, and will take your first pass with it using the monte-carlo œÄ program.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM with Distributed Work_](08_MCœÄ-NVSHMEM-Dist.ipynb): In this notebook you will expand your NVSHMEM skills by using it to distribute different work to multiple GPUs with NVSHMEM.\n",
    "- [_The NVSHMEM Memory Model_](09_MCœÄ-NVSHMEM-Sym.ipynb): In this notebook you will learn about NVSHMEM's symmetric memory - an elegant mechanism for inter-GPU communication initiated on the GPU - and will apply it to the monte-carlo œÄ program.\n",
    "- [_NVSHMEM Histogram: Duplicated Approach_](10_Histogram-Dup.ipynb): In this notebook you will learn how to use NVSHMEM to perform collective operations across GPUs using a histogram application.\n",
    "- [_NVSHMEM Histogram: Distributed Approach_](11_Histogram-Dist.ipynb): In this notebook you will take a different approach to the NVSHMEM histogram application and will learn how to reason about performance trade-offs in your multi GPU applications.\n",
    "- [_Jacobi Iteration_](12_Jacobi.ipynb): In this notebook you will be introduced to a Laplace equation solver using Jacobi iteration and will learn how to use NVSHMEM to handle boundary communications between multiple GPUs.\n",
    "- [_Improving the Reduction Performance with `cub`_](13_Jacobi-cub.ipynb): In this notebook you will learn about the `cub` library to improve the performance of your NVSHMEM Jacobi application.\n",
    "- [_Final Exercise_](14_Wave.ipynb): In this exercise you apply your day's learnings by refactoring a single GPU 1D wave equation solver to run on multiple GPUs with NVSHMEM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39406eeb",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4b818",
   "metadata": {},
   "source": [
    "Please continue to the next notebook: [_Monte Carlo Approximation of  ùúã  - Single GPU_](02_MCœÄ-SGPU.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
